\documentclass[12pt]{article}

\usepackage{graphics}
\usepackage{epsfig}
\usepackage{times}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{gensymb}
\usepackage{array}
\usepackage{float}
\usepackage{indentfirst}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{booktabs, makecell}
\usepackage{diagbox}
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage[table]{xcolor}

% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

\topmargin      0.0in
\headheight     0.0in
\headsep        0.0in
\oddsidemargin  0.0in
\evensidemargin 0.0in
\textheight     9.0in
\textwidth      6.5in

\title{{\bf SEDS536 Image Understanding \\ Term Project Report \\ Fall 2025} \\
	\it Skin Tone Detection for Inclusive Skincare Recommendations: \\ A Comparison of Classical and Deep Learning Approaches}

\date{\today}

\begin{document}

\pagestyle{plain}
\pagenumbering{roman}
\maketitle

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.35]{../project-materials/iyte_logo}
\end{figure}

\textbf{Student Information}
\begin{itemize}
	\item Umut Akin
\end{itemize}

\textbf{Source Code}
\begin{itemize}
	\item \url{https://github.com/umutakin-dev/seds536-term-project}
\end{itemize}

\cleardoublepage
\pagenumbering{arabic}

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
This project addresses the challenge of inclusive skin tone detection for personalized skincare recommendations, applying a comprehensive set of image understanding techniques. We compare classical image processing with deep learning approaches, evaluating their accuracy and fairness across diverse skin tones using the Monk Skin Tone Scale.

Our classical approach implements the Individual Typology Angle (ITA) method, demonstrating core image understanding techniques: color space conversion (RGB to YCbCr and LAB), skin segmentation via thresholding, morphological operations (opening and closing) for mask refinement, and face detection using both Haar Cascade classifiers and MediaPipe landmarks. We tested multiple segmentation methods including YCbCr color thresholding, face oval masking, and MediaPipe face landmarks.

Critically, these image understanding techniques were applied not only during analysis but also during \textbf{data preprocessing}---face detection and cropping removed background clutter, while skin mask generation created clean input for both pipelines.

For deep learning, we trained an EfficientNet-B0 model with transfer learning, optimized for on-device mobile inference to protect user privacy. The model was trained on Meta's Casual Conversations v2 dataset with 150,000+ images annotated using the 10-point Monk scale.

Results show that the CNN approach (78.6\% accuracy) dramatically outperforms classical ITA (52.3\% best case). We discovered that ITA is fundamentally unsuitable for fine-grained classification due to high variance under uncontrolled lighting---adjacent classes differ by only 3$\degree$ while standard deviations exceed 38$\degree$. Key challenges include severe class imbalance (Scale 5 has 394$\times$ more samples than Scale 10) and a 42\% accuracy gap between best and worst performing classes.

The final model achieves on-device inference in under 2 seconds with a 15MB TFLite model, meeting privacy requirements while providing practical skin tone classification for mobile applications.
\end{abstract}

\cleardoublepage

%==============================================================================
% INTRODUCTION
%==============================================================================
\section{Introduction}

\subsection{Problem Statement}
The skincare and cosmetics industry often fails to provide inclusive product recommendations for individuals across the full spectrum of skin tones. Many existing solutions either rely on subjective self-assessment or use classification systems that under-represent darker skin tones. This project aims to develop an automated, fair, and privacy-preserving skin tone detection system for mobile applications.

\subsection{Motivation}
Three key factors motivate this work:

\begin{enumerate}
    \item \textbf{Fairness}: Traditional skin classification systems like the Fitzpatrick scale \citep{fitzpatrick1988validity} were designed for UV sensitivity assessment, not skin color representation. Recent research \citep{buolamwini2018gender} has highlighted significant bias in computer vision systems, particularly for darker skin tones.

    \item \textbf{Privacy}: Users should not be required to upload facial images to remote servers for skin tone analysis. On-device processing ensures that sensitive biometric data never leaves the user's device.

    \item \textbf{Accessibility}: A mobile application with on-device ML can provide instant, offline skin tone analysis without requiring internet connectivity or subscription services.
\end{enumerate}

\subsection{Objectives}
The primary objectives of this project are:

\begin{enumerate}
    \item Implement and evaluate classical image processing techniques for skin tone classification, demonstrating course concepts including color space conversion, thresholding, and morphological operations.

    \item Develop a deep learning model that achieves high accuracy while being efficient enough for on-device mobile inference.

    \item Analyze fairness across skin tone categories, with a goal of minimizing accuracy disparities between light and dark skin tones.

    \item Compare classical and learned approaches to understand the limitations and strengths of each methodology.
\end{enumerate}

\subsection{Scope}
This project focuses on:
\begin{itemize}
    \item 3-class (Light/Medium/Dark) and 5-class skin tone classification
    \item Comparison of ITA-based classical methods vs CNN-based deep learning
    \item Evaluation using the Monk Skin Tone Scale \citep{monk2019skin}
    \item Mobile-optimized model deployment (TFLite)
\end{itemize}

%==============================================================================
% IMAGE UNDERSTANDING TECHNIQUES OVERVIEW
%==============================================================================
\section{Image Understanding Techniques Overview}

This project applies a comprehensive set of image understanding techniques from both classical and deep learning paradigms. Importantly, these techniques were applied at \textbf{two distinct stages}: (1) dataset preprocessing to clean and enhance raw video frames, and (2) skin tone analysis for classification. Table \ref{tab:techniques-overview} summarizes all techniques and their application stages.

\begin{table}[H]
\centering
\caption{Summary of Image Understanding Techniques Applied}
\label{tab:techniques-overview}
\begin{tabular}{p{2.8cm}p{3.5cm}p{4.2cm}p{2cm}}
\toprule
\textbf{Category} & \textbf{Technique} & \textbf{Application} & \textbf{Stage} \\
\midrule
\multirow{2}{2.8cm}{Face Detection}
    & Haar Cascade Classifier & Crop face region from raw frames & Preprocess \\
    & MediaPipe Face Landmarker & Extract precise face boundaries & Preprocess \\
\midrule
\multirow{2}{2.8cm}{Segmentation}
    & Face Region Cropping & Remove background, isolate face & Preprocess \\
    & Color Thresholding & Create skin-only binary masks & Both \\
\midrule
\multirow{2}{2.8cm}{Color Space Conversion}
    & RGB $\rightarrow$ YCbCr & Skin pixel detection via Cb/Cr & Analysis \\
    & RGB $\rightarrow$ CIE LAB & ITA calculation (L*, b* channels) & Analysis \\
\midrule
\multirow{2}{2.8cm}{Morphological Ops}
    & Opening (Erode+Dilate) & Remove noise from skin masks & Both \\
    & Closing (Dilate+Erode) & Fill holes in skin masks & Both \\
\midrule
\multirow{3}{2.8cm}{Deep Learning}
    & Transfer Learning & Leverage ImageNet features & Analysis \\
    & CNN (EfficientNet-B0) & Skin tone classification & Analysis \\
    & Data Augmentation & Improve robustness & Analysis \\
\midrule
Classical Formula
    & Individual Typology Angle & Colorimetric measurement & Analysis \\
\bottomrule
\end{tabular}
\end{table}

This two-stage pipeline ensures that both classical and deep learning models receive clean, face-focused input images rather than raw video frames with cluttered backgrounds. The following sections provide theoretical background (Section 3) and detailed implementation of each technique (Section 4).

%==============================================================================
% LITERATURE REVIEW
%==============================================================================
\section{Literature Review}

\subsection{Skin Tone Classification Scales}

\subsubsection{Fitzpatrick Scale}
The Fitzpatrick Skin Type scale \citep{fitzpatrick1988validity} classifies skin into six categories (I-VI) based on response to UV exposure. While widely used in dermatology, it has limitations for computer vision applications: it was designed for sun sensitivity rather than color representation, and its six categories under-represent the diversity of darker skin tones.

\subsubsection{Monk Skin Tone Scale}
The Monk Skin Tone (MST) scale \citep{monk2019skin} was developed specifically for machine learning fairness evaluation. It provides 10 categories with better representation across the full spectrum of human skin tones. We adopt this scale for our experiments, grouping into 3 classes (Light: 1-3, Medium: 4-7, Dark: 8-10) or 5 classes for different granularity levels.

\subsection{Classical Approaches: Individual Typology Angle (ITA)}

The Individual Typology Angle (ITA) is a dermatology standard for objective skin color measurement \citep{chardon1991skin, delbino2013variations}. It is calculated from the CIE LAB color space:

\begin{equation}
    ITA = \arctan\left(\frac{L^* - 50}{b^*}\right) \times \frac{180}{\pi}
\end{equation}

where $L^*$ represents lightness and $b^*$ represents the yellow-blue axis. Higher ITA values indicate lighter skin. Standard thresholds classify skin as Very Light ($>55\degree$), Light ($41\degree$-$55\degree$), Intermediate ($28\degree$-$41\degree$), Tan ($10\degree$-$28\degree$), or Dark ($<10\degree$).

\subsection{Skin Segmentation Techniques}

Effective ITA calculation requires isolating skin pixels from non-skin regions (hair, eyes, background). Common approaches include:

\begin{itemize}
    \item \textbf{Color-based segmentation}: YCbCr color space thresholding is widely used for skin detection \citep{vezhnevets2003survey}, as skin colors cluster in a relatively compact region of the Cb-Cr plane regardless of ethnicity.

    \item \textbf{Face detection}: Haar Cascade classifiers \citep{viola2004robust} provide fast face detection but produce coarse bounding boxes. MediaPipe Face Mesh \citep{lugaresi2019mediapipe} provides 478 facial landmarks for precise face region extraction.

    \item \textbf{Morphological operations}: Opening (erosion followed by dilation) removes noise, while closing (dilation followed by erosion) fills holes \citep{gonzalez2009digital}.
\end{itemize}

\subsection{Deep Learning Approaches}

Convolutional Neural Networks (CNNs) have largely replaced classical methods for image classification tasks. Transfer learning \citep{weiss2016survey} enables training effective models even with limited data by leveraging features learned from large datasets like ImageNet \citep{deng2009imagenet}.

EfficientNet \citep{tan2019efficientnet} provides an excellent accuracy-efficiency tradeoff through compound scaling of network depth, width, and resolution. EfficientNet-B0, with only 4 million parameters, is suitable for mobile deployment while achieving strong classification performance.

\subsection{Fairness in Machine Learning}

\cite{buolamwini2018gender} demonstrated significant accuracy disparities in commercial face analysis systems, with error rates up to 34\% higher for darker-skinned females compared to lighter-skinned males. \cite{mehrabi2021survey} provide a comprehensive survey of bias sources and mitigation strategies in ML systems.

Key fairness metrics include:
\begin{itemize}
    \item \textbf{Accuracy parity}: Similar accuracy across demographic groups
    \item \textbf{Equalized odds}: Similar true positive and false positive rates
    \item \textbf{Worst-case performance}: Minimum accuracy across all groups
\end{itemize}

%==============================================================================
% METHODOLOGY
%==============================================================================
\section{Methodology}

This section details the implementation of image understanding techniques summarized in Table \ref{tab:techniques-overview}. We first describe the data preprocessing pipeline (Section 4.2) where face detection, cropping, and mask generation were applied to clean and enhance the raw dataset. We then present two parallel analysis approaches: a classical image processing pipeline (Section 4.3) demonstrating color space conversion, thresholding, and morphological operations; and a deep learning approach (Section 4.4) leveraging transfer learning and convolutional neural networks. Both approaches were evaluated on the preprocessed dataset variants to enable direct comparison.

\subsection{Dataset}

We use the Casual Conversations v2 (CCv2) dataset \citep{porgali2023casual} from Meta AI Research. This dataset contains video frames with Monk Skin Tone scale annotations (1-10).

\begin{table}[H]
\centering
\caption{Dataset Statistics}
\begin{tabular}{lrrr}
\toprule
\textbf{Split} & \textbf{Images} & \textbf{Purpose} \\
\midrule
Train & 104,510 & Model training \\
Validation & 22,280 & Hyperparameter tuning \\
Test & 22,730 & Final evaluation \\
\bottomrule
\end{tabular}
\end{table}

The dataset exhibits severe class imbalance: Scale 5 (Medium) contains 45\% of all samples, while Scale 10 (Darkest) contains only 0.1\%. This 394$\times$ imbalance presents a significant challenge for fair classification.

\subsection{Data Preprocessing Pipeline}

Raw video frames from CCv2 contain full scenes with varied backgrounds, multiple subjects, and inconsistent framing. Before applying analysis techniques, we preprocessed the dataset using image understanding methods to create clean, face-focused images.

\subsubsection{Face Detection and Cropping}
We applied face detection to extract face regions from raw frames:
\begin{itemize}
    \item \textbf{Haar Cascade Detection}: Used OpenCV's pre-trained frontal face classifier to detect face bounding boxes. Detected regions were cropped with padding to ensure the full face was captured.
    \item \textbf{MediaPipe Face Landmarker}: For higher-precision preprocessing, we used MediaPipe to detect 478 facial landmarks and extract tight face boundaries.
\end{itemize}

This step removed background clutter and ensured consistent input for both classical and deep learning pipelines.

\subsubsection{Skin Mask Generation for Dataset Enhancement}
For experiments requiring skin-only analysis, we generated binary skin masks:
\begin{enumerate}
    \item Convert cropped face images from RGB to YCbCr color space
    \item Apply color thresholding (Cb: 77-127, Cr: 133-173) to identify skin pixels
    \item Apply morphological opening (5$\times$5 elliptical kernel) to remove noise
    \item Apply morphological closing to fill small holes in the mask
    \item Store both the cropped images and corresponding masks
\end{enumerate}

\subsubsection{Preprocessed Dataset Variants}
The preprocessing pipeline produced multiple dataset variants for experimentation:
\begin{table}[H]
\centering
\caption{Preprocessed Dataset Variants}
\begin{tabular}{llc}
\toprule
\textbf{Variant} & \textbf{Preprocessing Applied} & \textbf{Used In} \\
\midrule
Raw frames & None & Exp 1 (baseline) \\
Haar face crops & Face detection + crop & Exp 2, 3 \\
MediaPipe crops & Landmark-based crop & Exp 4, 5 \\
MediaPipe + masks & Crop + skin segmentation & Exp 3e, 4 \\
\bottomrule
\end{tabular}
\end{table}

This preprocessing approach demonstrates that image understanding techniques are valuable not only for final analysis but also for dataset preparation---enabling fair comparison between methods by providing consistent, clean input data.

\subsection{Classical Approach: ITA with Skin Segmentation}

Our classical pipeline implements the following steps:

\subsubsection{Step 1: Face Detection}
We tested two face detection methods:
\begin{itemize}
    \item \textbf{Haar Cascade}: OpenCV's pre-trained frontal face detector \citep{viola2004robust}. Fast but produces coarse bounding boxes.
    \item \textbf{MediaPipe Face Landmarker}: Provides 478 facial landmarks for precise face region extraction \citep{lugaresi2019mediapipe}.
\end{itemize}

\subsubsection{Step 2: Color Space Conversion}
Convert from RGB to YCbCr for skin detection:
\begin{lstlisting}[language=Python]
image_ycbcr = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YCrCb)
\end{lstlisting}

\subsubsection{Step 3: Skin Thresholding}
Apply thresholds to isolate skin pixels:
\begin{lstlisting}[language=Python]
# YCbCr skin thresholds (literature-based)
Cb_range = [77, 127]
Cr_range = [133, 173]
mask = ((Cb >= 77) & (Cb <= 127) &
        (Cr >= 133) & (Cr <= 173))
\end{lstlisting}

\subsubsection{Step 4: Morphological Operations}
Apply opening and closing to clean the mask:
\begin{lstlisting}[language=Python]
kernel = cv2.getStructuringElement(
    cv2.MORPH_ELLIPSE, (5, 5))
# Opening: removes small noise
mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
# Closing: fills small holes
mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
\end{lstlisting}

\subsubsection{Step 5: ITA Calculation}
Convert to LAB color space and compute ITA on skin pixels:
\begin{lstlisting}[language=Python]
image_lab = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2LAB)
L = image_lab[:,:,0] * 100.0 / 255.0  # Scale to [0,100]
b = image_lab[:,:,2] - 128.0  # Center at 0
L_mean = np.mean(L[skin_mask])
b_mean = np.mean(b[skin_mask])
ITA = np.arctan((L_mean - 50) / b_mean) * (180 / np.pi)
\end{lstlisting}

\subsubsection{Step 6: Threshold-based Classification}
Map ITA values to classes using tuned thresholds:
\begin{itemize}
    \item 3-class: Light ($>25\degree$), Medium ($-50\degree$ to $25\degree$), Dark ($<-50\degree$)
    \item Thresholds tuned via grid search on validation set
\end{itemize}

\subsection{Deep Learning Approach: EfficientNet-B0}

\subsubsection{Architecture}
We use EfficientNet-B0 \citep{tan2019efficientnet} pre-trained on ImageNet \citep{deng2009imagenet}:
\begin{itemize}
    \item Input: 224$\times$224$\times$3 RGB images
    \item Backbone: EfficientNet-B0 (4M parameters)
    \item Head: Dropout (0.3) $\rightarrow$ Fully Connected $\rightarrow$ Softmax
    \item Output: 3 or 5 class probabilities
\end{itemize}

\subsubsection{Training Configuration}
\begin{table}[H]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Optimizer & AdamW \\
Learning Rate & 0.001 (cosine decay) \\
Batch Size & 32 \\
Epochs & 30 \\
Weight Decay & 0.0001 \\
Early Stopping & Patience = 7 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Class Imbalance Handling}
\begin{itemize}
    \item \textbf{Weighted Loss}: Cross-entropy with inverse frequency class weights
    \item \textbf{Oversampling}: WeightedRandomSampler to balance mini-batches
    \item \textbf{Data Augmentation}: Random horizontal flip, rotation ($\pm15\degree$), color jitter (brightness, contrast, saturation, hue)
\end{itemize}

\subsubsection{Model Conversion}
For on-device deployment, we convert the PyTorch model to TensorFlow Lite:
\begin{itemize}
    \item Tool: ai-edge-torch (Google's PyTorch to TFLite converter)
    \item Output size: $\sim$15MB
    \item Inference time: $<$2 seconds on mid-range mobile devices
\end{itemize}

%==============================================================================
% EXPERIMENTS & RESULTS
%==============================================================================
\section{Experiments \& Results}

\subsection{Experiment Overview}

We conducted six experiments systematically varying the method, preprocessing, and number of classes:

\begin{table}[H]
\centering
\caption{Experiment Summary}
\begin{tabular}{clllc}
\toprule
\textbf{Exp} & \textbf{Method} & \textbf{Preprocessing} & \textbf{Classes} & \textbf{Test Acc} \\
\midrule
1 & CNN & None & 10 & 38.7\% \\
2 & CNN & Haar face crop & 3 & \textbf{78.6\%} \\
3 & ITA & Various & 3 & 52.3\% \\
4 & CNN & MediaPipe masks & 3 & 77.1\% \\
5a & ITA & MediaPipe raw & 5 & 17.1\% \\
5 & CNN & MediaPipe raw & 5 & 62.5\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experiment 1: 10-Class CNN Baseline}

Training a 10-class classifier on the original Monk scale failed due to severe class imbalance:
\begin{itemize}
    \item Test accuracy: 38.7\%
    \item Scale 10 (darkest): 0\% accuracy (complete failure)
    \item Model biased toward majority classes (scales 3-6)
\end{itemize}

\subsection{Experiment 2: 3-Class CNN with Haar Face Crops (Best Model)}

Grouping into 3 classes and using face-cropped images dramatically improved results:

\begin{table}[H]
\centering
\caption{Experiment 2: Per-Class Performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
\midrule
Light (1-3) & 0.517 & 0.609 & 0.560 & 4,530 \\
Medium (4-7) & 0.881 & 0.844 & 0.862 & 17,570 \\
Dark (8-10) & 0.486 & 0.424 & 0.453 & 630 \\
\midrule
\textbf{Overall} & \multicolumn{4}{c}{\textbf{78.6\% accuracy, 0.625 Macro F1}} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experiment 3: ITA Classical Baseline}

We tested ITA with multiple segmentation methods:

\begin{table}[H]
\centering
\caption{ITA Experiments Comparison}
\begin{tabular}{llc}
\toprule
\textbf{Variant} & \textbf{Segmentation} & \textbf{Test Acc} \\
\midrule
3a & None (raw) & 19.3\% \\
3b & YCbCr color + tuned thresholds & 52.3\% \\
3c & Face oval + color & 52.2\% \\
3d & Smaller oval (0.5$\times$0.7) & 52.3\% \\
3e & MediaPipe landmarks & 53.7\% \\
\bottomrule
\end{tabular}
\end{table}

Key finding: ITA accuracy plateaus around 52-54\% regardless of segmentation method. The bottleneck is the ITA formula itself, not the preprocessing.

\subsection{Experiment 5a: 5-Class ITA with Empirical Thresholds}

Testing 5-class ITA revealed a critical finding:

\begin{table}[H]
\centering
\caption{ITA Medians by Monk Class (5-class)}
\begin{tabular}{lccc}
\toprule
\textbf{Class} & \textbf{Median ITA} & \textbf{Std Dev} & \textbf{Gap to Next} \\
\midrule
Very Light & 25.8$\degree$ & 37.0$\degree$ & 15.1$\degree$ \\
Light & 10.7$\degree$ & 37.7$\degree$ & 9.6$\degree$ \\
Medium & 1.0$\degree$ & 39.4$\degree$ & \textbf{3.1$\degree$} \\
Dark & -2.1$\degree$ & 38.9$\degree$ & 66.4$\degree$ \\
Very Dark & -68.5$\degree$ & 45.7$\degree$ & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical Discovery}: While ITA medians follow the expected order, the Medium and Dark classes are only \textbf{3.1$\degree$ apart}---yet both have standard deviations exceeding \textbf{38$\degree$}. This massive overlap (distributions span $\sim$180$\degree$ each) makes threshold-based separation between adjacent classes impossible. ITA's high variance under uncontrolled lighting renders it ineffective for fine-grained skin tone classification.

\subsection{Key Results Summary}

\begin{table}[H]
\centering
\caption{Overall Results Comparison}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Classes} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Training Time} \\
\midrule
ITA (best classical) & 3 & 52.3\% & 0.337 & 0 hours \\
ITA (5-class) & 5 & 17.1\% & 0.155 & 0 hours \\
\textbf{CNN 3-class (best)} & 3 & \textbf{78.6\%} & \textbf{0.625} & 2.4 hours \\
CNN 5-class & 5 & 62.5\% & -- & 7.5 hours \\
CNN 10-class & 10 & 38.7\% & 0.230 & 6.2 hours \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Fairness Analysis}

\begin{table}[H]
\centering
\caption{Fairness Metrics (Best Model: Exp 2)}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Target} \\
\midrule
Accuracy Gap (best - worst) & 42.0\% & $<$10\% \\
Worst-Case Ratio & 50.2\% & $>$85\% \\
Best Class (Medium) & 84.4\% & -- \\
Worst Class (Dark) & 42.4\% & -- \\
\bottomrule
\end{tabular}
\end{table}

Fairness targets are not met due to data imbalance: Medium class has 77\% of training data while Dark has only 2.5\%. However, significant progress was made---Dark class improved from 0\% (Exp 1) to 42.4\% (Exp 2).

%==============================================================================
% DISCUSSION
%==============================================================================
\section{Discussion}

\subsection{Why CNN Outperforms ITA}

The CNN achieves 78.6\% accuracy compared to ITA's 52.3\% ceiling because:

\begin{enumerate}
    \item \textbf{Learned vs. Hand-crafted Features}: CNNs learn task-specific features from data, while ITA uses a fixed colorimetric formula designed for clinical settings.

    \item \textbf{Perceptual vs. Colorimetric}: The Monk scale is based on visual perception, not the $L^*/b^*$ ratio. CNNs can learn perceptual features that align with human annotation.

    \item \textbf{Lighting Invariance}: CNNs learn robust features through augmentation, while ITA is sensitive to lighting variation.

    \item \textbf{High Variance}: ITA values have standard deviations exceeding 38$\degree$ per class, causing massive overlap between adjacent classes. The 3$\degree$ gap between Medium and Dark medians is dwarfed by their $\sim$39$\degree$ standard deviations, making threshold-based classification unreliable.
\end{enumerate}

\subsection{Preprocessing: Simpler is Better}

Surprisingly, simpler Haar face crops (78.6\%) outperformed sophisticated MediaPipe skin masks (77.1\%). This is because:
\begin{itemize}
    \item CNNs naturally learn to focus on relevant regions
    \item MediaPipe had 12\% detection failures, reducing training data
\end{itemize}

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{Class Imbalance}: 42\% accuracy gap between best and worst classes
    \item \textbf{Single Dataset}: Results may not generalize to other populations
    \item \textbf{Uncontrolled Lighting}: CCv2 contains varied smartphone lighting
    \item \textbf{Privacy Trade-off}: On-device models have less capacity than cloud models
\end{enumerate}

%==============================================================================
% CONCLUSION
%==============================================================================
\section{Conclusion}

This project demonstrated both classical and deep learning approaches to skin tone classification, achieving the primary goal of on-device inference for privacy-preserving mobile applications.

\vspace{1em}
\textbf{Key Contributions}:
\begin{enumerate}
    \item Application of image understanding techniques at multiple stages: data preprocessing (face detection, cropping, mask generation) and analysis (segmentation, classification)
    \item Systematic comparison of ITA (classical) vs CNN (deep learning) approaches
    \item Discovery that ITA's high variance makes it unsuitable for fine-grained classification
    \item Mobile-optimized model achieving 78.6\% accuracy in 15MB
    \item Comprehensive fairness analysis across skin tone categories
\end{enumerate}

\textbf{Image Understanding Techniques Demonstrated}:
\begin{itemize}
    \item \textbf{Face Detection} (Preprocessing): Haar Cascade and MediaPipe for face region extraction from raw video frames
    \item \textbf{Segmentation} (Preprocessing + Analysis): Face cropping for dataset cleanup; color thresholding for skin pixel isolation
    \item \textbf{Color Space Conversion} (Analysis): RGB $\rightarrow$ YCbCr for skin detection, RGB $\rightarrow$ LAB for ITA calculation
    \item \textbf{Morphological Operations} (Both): Opening/closing for mask refinement in preprocessing and analysis
    \item \textbf{Deep Learning} (Analysis): Transfer learning with EfficientNet-B0, data augmentation for robustness
    \item \textbf{Classical Feature Extraction} (Analysis): Individual Typology Angle (ITA) formula for colorimetric measurement
\end{itemize}

\textbf{Future Work}:
\begin{itemize}
    \item Balanced data augmentation to equalize all class sizes
    \item Focal loss for improved class balance
    \item Confidence thresholding to reject uncertain predictions
    \item MLOps pipelines (MLflow) for experiment tracking and model versioning
    \item User testing with diverse participants
\end{itemize}

%==============================================================================
% PROJECT SCHEDULE
%==============================================================================
\section{Project Schedule}

\begin{table}[H]
\centering
\caption{Project Timeline (Green = Complete, Yellow = In Progress)}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Task} & \textbf{W1-2} & \textbf{W3-4} & \textbf{W5-6} & \textbf{W7-8} & \textbf{W9-10} & \textbf{W11+} \\
\hline
Literature Review & \cellcolor{green!30}X & & & & & \\
\hline
Dataset Preparation & \cellcolor{green!30}X & \cellcolor{green!30}X & & & & \\
\hline
ITA Implementation & & \cellcolor{green!30}X & \cellcolor{green!30}X & & & \\
\hline
CNN Training & & & \cellcolor{green!30}X & \cellcolor{green!30}X & & \\
\hline
Experiments & & & & \cellcolor{green!30}X & \cellcolor{green!30}X & \\
\hline
Model Conversion & & & & & \cellcolor{green!30}X & \\
\hline
Report \& Presentation & & & & & & \cellcolor{green!30}X \\
\hline
\end{tabular}
\end{table}

%==============================================================================
% BIBLIOGRAPHY
%==============================================================================
\bibliographystyle{chicago}
\bibliography{literature}

\end{document}
